{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#General purpose\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Cross-validation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#Models\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_train = pd.read_csv(\"data/train.csv\")\n",
    "corpus_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define challenge loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_loss(y_true, y_pred):\n",
    "    return np.mean([log_loss(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get insults from external dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insults = pd.read_csv('data/insults.txt', names=[\"words\"])\n",
    "insults_dict = {v:k for k,v in insults[\"words\"].to_dict().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns extraction for easy usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = corpus_train.columns[2:]\n",
    "train_text = corpus_train[\"comment_text\"]\n",
    "test_text = corpus_test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train(70%)/validation(30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 111699\n",
      "Test size: 47872\n"
     ]
    }
   ],
   "source": [
    "xtrain, xval, ytrain, yval = train_test_split(corpus_train[\"comment_text\"],\n",
    "                                                    corpus_train[outputs],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42) #for reproduciblity\n",
    "\n",
    "print(\"Train size: {}\".format(xtrain.shape[0]))\n",
    "print(\"Test size: {}\".format(xval.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting DF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=5000,\n",
    "                            analyzer='word',\n",
    "                            min_df=10,\n",
    "                            strip_accents='unicode',\n",
    "                            token_pattern=r'\\w{1,}',\n",
    "                            #token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "                            ngram_range=(1,1)\n",
    "                           ).fit(train_text.append(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_df = count_vec.transform(xtrain)\n",
    "xval_df = count_vec.transform(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000,\n",
    "                        min_df=5, \n",
    "                        strip_accents='unicode',\n",
    "                        analyzer='word',\n",
    "                        #analyzer='char', #works also with chars!\n",
    "                        #token_pattern=r'\\w{1,}',\n",
    "                        token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "                        #ngram_range=(1, 2), #ng_grams don't really improve results here\n",
    "                        use_idf=1,\n",
    "                        smooth_idf=1,\n",
    "                        sublinear_tf=1,\n",
    "                        stop_words = 'english',\n",
    "                        lowercase=False\n",
    "                       ).fit(train_text.append(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_tfidf = tfidf.transform(xtrain)\n",
    "xval_tfidf = tfidf.transform(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive bayes - DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2400\n"
     ]
    }
   ],
   "source": [
    "naive = OneVsRestClassifier(MultinomialNB(alpha=0.1)).fit(xtrain_df, ytrain)\n",
    "y_pred = naive.predict_proba(xval_df)\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Naive bayes - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0796\n"
     ]
    }
   ],
   "source": [
    "naive = OneVsRestClassifier(MultinomialNB(alpha=0.1)).fit(xtrain_tfidf, ytrain)\n",
    "y_pred = naive.predict_proba(xval_tfidf)\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No cross-validation for now, see later\n",
    "lr_ovr = OneVsRestClassifier(LogisticRegression(C=10)).fit(xtrain_df, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0834\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_ovr.predict_proba(xval_df)\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_ovr = OneVsRestClassifier(LogisticRegression(C=5)).fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0553\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_ovr.predict_proba(xval_tfidf)\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_chains = ClassifierChain(LogisticRegression(C=5), order='random').fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0588\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_chains.predict_proba(xval_tfidf)\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble classifier chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ECC (Ensemble chain classifier)\n",
    "def ECC_preds_multiple_mods(x, mods):\n",
    "    return np.array([voter.predict(x) for voter in mods])\n",
    "\n",
    "def ECC_predict(predictions):\n",
    "    nb_voters = predictions.shape[0]\n",
    "    return (sum([x for x in predictions])/nb_voters >= 0.5).astype(int)\n",
    "\n",
    "def ECC_fit(xtrain, ytrain, model, nb_models=5):\n",
    "    np.random.seed()\n",
    "    models = []\n",
    "    for i in range(nb_models):\n",
    "        models.append(ClassifierChain(clone(model), order='random').fit(xtrain, ytrain))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_ecc = ECC_fit(xtrain_tfidf, ytrain, LogisticRegression(C=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_preds_ecc = ECC_predict(ECC_preds_multiple_mods(xval_tfidf, lr_ecc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6686\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_preds_ecc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a simple xgboost on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0629\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=4, learning_rate=0.1\n",
    "                       )).fit(xtrain_tfidf.tocsc(), ytrain)\n",
    "y_pred = clf.predict_proba(xval_tfidf.tocsc())\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a simple xgboost on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0563\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=4, learning_rate=0.1\n",
    "                       )).fit(xtrain_df.tocsc(), ytrain)\n",
    "\n",
    "y_pred = clf.predict_proba(xval_df.tocsc())\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a simple xgboost on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0629\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = OneVsRestClassifier(xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=4, learning_rate=0.1\n",
    "                       )).fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "y_pred = clf.predict_proba(xval_tfidf)\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scorer = make_scorer(calc_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example with SVD, scaling features + learning logistic model\n",
    "# Initialize SVD\n",
    "#svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "#scl = StandardScaler()\n",
    "\n",
    "lr_ovr = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Create the pipeline, we dont need a full pipeline here, just cross-validate\n",
    "# the regularization parameter and the norm to have an idea of possible improvements\n",
    "#clf = Pipeline([('svd', svd), ('scl', scl), ('lr_ovr', lr_ovr)])\n",
    "clf = Pipeline([('lr_ovr', lr_ovr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_ovr',\n",
       " 'lr_ovr__estimator',\n",
       " 'lr_ovr__estimator__C',\n",
       " 'lr_ovr__estimator__class_weight',\n",
       " 'lr_ovr__estimator__dual',\n",
       " 'lr_ovr__estimator__fit_intercept',\n",
       " 'lr_ovr__estimator__intercept_scaling',\n",
       " 'lr_ovr__estimator__max_iter',\n",
       " 'lr_ovr__estimator__multi_class',\n",
       " 'lr_ovr__estimator__n_jobs',\n",
       " 'lr_ovr__estimator__penalty',\n",
       " 'lr_ovr__estimator__random_state',\n",
       " 'lr_ovr__estimator__solver',\n",
       " 'lr_ovr__estimator__tol',\n",
       " 'lr_ovr__estimator__verbose',\n",
       " 'lr_ovr__estimator__warm_start',\n",
       " 'lr_ovr__n_jobs',\n",
       " 'memory',\n",
       " 'steps']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List all possible paramaters for the classifiers at hand\n",
    "sorted(clf.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define values to cross validate\n",
    "param_grid = {'lr_ovr__estimator__C': [0.1, 1, 10], \n",
    "              'lr_ovr__estimator__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  4.1min finished\n"
     ]
    }
   ],
   "source": [
    "# Initialize Grid Search Model\n",
    "#Use refit=True to get a model with the best paramaters on the full training\n",
    "model = GridSearchCV(estimator=clf,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring=cv_scorer,\n",
    "                     verbose=1,\n",
    "                     n_jobs=1,\n",
    "                     iid=False,\n",
    "                     refit=True,\n",
    "                     cv=3).fit(xtrain_tfidf, ytrain.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.057\n",
      "Best parameters set:\n",
      "\tlr_ovr__estimator__C: 1\n",
      "\tlr_ovr__estimator__penalty: 'l1'\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive bayes cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scorer = make_scorer(calc_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_model = OneVsRestClassifier(MultinomialNB())\n",
    "\n",
    "# Create the pipeline \n",
    "clf = Pipeline([('nb', nb_model)])\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {'nb__estimator__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['memory',\n",
       " 'nb',\n",
       " 'nb__estimator',\n",
       " 'nb__estimator__alpha',\n",
       " 'nb__estimator__class_prior',\n",
       " 'nb__estimator__fit_prior',\n",
       " 'nb__n_jobs',\n",
       " 'steps']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List all possible paramaters for the classifiers at hand\n",
    "sorted(clf.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    2.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Fit Grid Search Model\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring=cv_scorer,\n",
    "                     verbose=1,\n",
    "                     n_jobs=-1,\n",
    "                     iid=True,\n",
    "                     refit=True,\n",
    "                     cv=2).fit(xtrain_tfidf, ytrain.as_matrix())\n",
    "# we could use the full data here but im only using xtrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.073\n",
      "Best parameters set:\n",
      "\tnb__estimator__alpha: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:32, 23618.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary\n",
    "from tqdm import tqdm\n",
    "embeddings_index = {}\n",
    "#Choose a pre-trained embedding here\n",
    "f = open('data/glove.840B.300d.txt')\n",
    "#f = open('data/glove.twitter.27B/glove.twitter.27B.25d.txt') \n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sloan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sloan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "rep_size = embeddings_index[\"and\"].shape[0]\n",
    "\n",
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]#filter words that are not in stop_word list\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(rep_size)\n",
    "    \n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111699/111699 [01:23<00:00, 1336.36it/s]\n",
      "100%|██████████| 47872/47872 [00:35<00:00, 1363.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.0570 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple lr on glove features\n",
    "clf = OneVsRestClassifier(LogisticRegression(C=5))\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.4f \" % calc_loss(np.array(yval), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = OneVsRestClassifier(xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False))\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.4f \" % calc_loss(np.array(yval), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting simple 2 layers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#useful variables\n",
    "EMB_SIZE = xtrain_glove.shape[1]\n",
    "OUTPUT_SIZE = ytrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=EMB_SIZE, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain,\n",
    "          batch_size=128,\n",
    "          callbacks=[earlyStopping],\n",
    "          validation_split=0.1,\n",
    "          epochs=100,\n",
    "          verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict_proba(xvalid_glove_scl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = ytrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here: need to tokenize text to apply LSTM\n",
    "token = Tokenizer(num_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token.fit_on_texts(list(xtrain) + list(xval))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zero pad the sequences -> can improve here\n",
    "max_len = 140\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare test\n",
    "xtest_seq = token.texts_to_sequences(corpus_test[\"comment_text\"])\n",
    "xtest_pad = pad_sequences(xtest_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1,\n",
    "                     output_dim=300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=xtrain_pad, y=ytrain, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yval), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handmade features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#subjective --> not used yet\n",
    "punctuation = list(\"?.,!:-;|\") + [\"..\"] + [\"...\"] + ['\"']\n",
    "biased_punct = [\"!\"] + [\"...\"]\n",
    "#smileys = [\":-)\", \":)\", \":(\", \":|\", \":-(\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "    #df_out = pd.DataFrame(df[\"comment_text\"].apply(lambda x: sum([s in x for s in smileys])), columns=[\"n_smileys\"])\n",
    "    df_out = pd.DataFrame(df[\"comment_text\"].apply(lambda x: len(x)), columns=[\"n_carac\"])\n",
    "    df_out[\"n_upper\"] = df[\"comment_text\"].apply(lambda x: sum(a.isupper() for a in list(x)))\n",
    "    df_out[\"n_ratio\"] = df[\"comment_text\"].apply(lambda x: float(sum(a.isupper() for a in list(x))) / len(x))\n",
    "    df_out[\"n_words\"] = df[\"comment_text\"].apply(lambda x: len(x.split(' '))\n",
    "    df_out[\"n_smileys\"] = df[\"comment_text\"].apply(lambda x: sum([s in x for s in smileys]))\n",
    "    df_out[\"n_carac\"] = df[\"comment_text\"].apply(lambda x: len(x))\n",
    "    \n",
    "    for p in punctuation:\n",
    "        new_entry = \"n_\" + str(p)\n",
    "        df_out[new_entry] = df[\"comment_text\"].str.count(\"\\\\\"+ p)\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get insults from external dictionnary\n",
    "insults_google = pd.read_csv('data/insults_google.txt', names=[\"words\"])\n",
    "insults_others = pd.read_csv('data/insults.txt', names=['words'])\n",
    "insults_set = set({v:k for k,v in insults_google[\"words\"].to_dict().items()})\n",
    "#insults_set = set({v:k for k,v in insults_others[\"words\"].to_dict().items()})\n",
    "insults_set.update(set({v:k for k,v in insults_others[\"words\"].to_dict().items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_df = count_vec.transform(xtrain)\n",
    "xval_df = count_vec.transform(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_inv_count = count_vec.inverse_transform(xtrain_df)\n",
    "xval_inv_count = count_vec.inverse_transform(xval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insults_train = pd.Series([int(len(set(xtrain_inv_count[i]).intersection(insults_set))>=1) for i in range(len(xtrain))], index=ytrain.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insults_val = pd.Series([int(len(set(xval_inv_count[i]).intersection(insults_set))>=1) for i in range(len(xval))], index=yval.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in ytrain.keys():\n",
    "    print k, \":\", accuracy_score(insults_train, ytrain[k]), ',', precision_score(insults_train, ytrain[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No cross-validation with logistic regression\n",
    "lr_ovr = OneVsRestClassifier(LogisticRegression(C=10)).fit(xtrain_tfidf, ytrain)\n",
    "y_pred = lr_ovr.predict_proba(xval_tfidf)\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(y_pred, columns=outputs, index=yval.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp.sparse.coo_matrix(insults_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_xtrain = sp.sparse.hstack([xtrain_tfidf, sp.sparse.coo_matrix(insults_train).T])\n",
    "imp_xval = sp.sparse.hstack([xval_tfidf, sp.sparse.coo_matrix(insults_val).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No cross-validation with logistic regression\n",
    "lr_ovr = OneVsRestClassifier(LogisticRegression(C=10)).fit(imp_xtrain, ytrain)\n",
    "y_pred = lr_ovr.predict_proba(imp_xval)\n",
    "print(\"Loss: %0.4f\" %calc_loss(yval.as_matrix(), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preproc for challenge\n",
    "import scipy as sp\n",
    "xtrain_tfidf = tfidf.transform(train_text)\n",
    "xtest_tfidf = tfidf.transform(test_text)\n",
    "xtrain_df = count_vec.transform(train_text)\n",
    "xtest_df = count_vec.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_inv_count = count_vec.inverse_transform(xtrain_df)\n",
    "xtest_inv_count = count_vec.inverse_transform(xtest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insults_train = pd.Series([int(len(set(xtrain_inv_count[i]).intersection(insults_set))>=1) \n",
    "                           for i in range(len(train_text))], index=corpus_train['id'])\n",
    "insults_test = pd.Series([int(len(set(xtest_inv_count[i]).intersection(insults_set))>=1) \n",
    "                         for i in range(len(test_text))], index=corpus_test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_xtrain = sp.sparse.hstack([xtrain_tfidf, sp.sparse.coo_matrix(insults_train).T])\n",
    "imp_xtest = sp.sparse.hstack([xtest_tfidf, sp.sparse.coo_matrix(insults_test).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_ovr = OneVsRestClassifier(LogisticRegression(C=10)).fit(imp_xtrain, corpus_train[outputs])\n",
    "y_pred = lr_ovr.predict_proba(imp_xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF on full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_train = tfidf.transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_test = tfidf.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_ovr = OneVsRestClassifier(LogisticRegression(C=10)).fit(tfidf_train, corpus_train[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = lr_ovr.predict_proba(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GloVe representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = count_vec.transform(train_text)\n",
    "df_test = count_vec.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(corpus_train[\"comment_text\"])]\n",
    "xtest_glove = [sent2vec(x) for x in tqdm(corpus_test[\"comment_text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xtest_glove = np.array(xtest_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xtest_glove_scl = scl.transform(xtest_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(xtrain_glove_scl, y=corpus_train[outputs], batch_size=64, callbacks=[earlyStopping], validation_split=0.1,\n",
    "          epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(xtest_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_train.loc[:, outputs].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit model on full training\n",
    "mod_full = OneVsRestClassifier(LogisticRegression(C=5)).fit(xtrain_glove,  corpus_train[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_full = OneVsRestClassifier(xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=4, learning_rate=0.1\n",
    "                       )).fit(df_train,  np.array(corpus_train[outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = mod_full.predict_proba(xtest_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_compet(corp, predictions, filename):\n",
    "    df_output = pd.concat([corp[\"id\"], pd.DataFrame(predictions, columns=[outputs])], axis=1)\n",
    "    df_output.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_compet(corpus_test, y_pred, \"lr_insultsEng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
